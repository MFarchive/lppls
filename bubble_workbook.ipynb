{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444cdd1",
   "metadata": {},
   "source": [
    "### Workbook: Identify Bubbles in FTSE AIM Index\n",
    "\n",
    "Layout of this workbook into the following sections\n",
    "\n",
    "01. Section to import tickers for the FTSE AIM Index\n",
    "   \n",
    "02. Choosing which trends to identify: positive bubbles, negative bubbles, etc.\n",
    "\n",
    "03. Running the analysis\n",
    "\n",
    "04. Presenting Results of analysis\n",
    "\n",
    "05. Downloading Results in correct format for a report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40f689",
   "metadata": {},
   "source": [
    "**How the model works**\n",
    "\n",
    "- The model works by only looking at historical price data of a security\n",
    "- The model takes historical price data of a stock, over a fixed period\n",
    "- Then tries to fit a curve to the data over varying time windows\n",
    "- The curves look for future bubbles in the stock price (for example positive bubbles where price increases rapidly)\n",
    "- Time windows are calculated by dividing the historical data into segments (i.e 8 individual weeks of a 2 month dataset)\n",
    "- The confidence in the future projected trend is determined by analysing successive segments of time series data and seeing how many of them give the same extrapolated fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700052c",
   "metadata": {},
   "source": [
    "**Purpose of the report**\n",
    "\n",
    "I have two ideas in mind for the report, a report on publically traded insurance company stocks or a report on the small/micro cap FTSE AIM Index. Both sectors presumably have little analysis produced on them, and a report may add value and or be able to identify trends in prices.\n",
    "\n",
    "- A report can be produced for either group\n",
    "- The report will highlight if certain companies show evidence that there is a bubble forming in the stock price\n",
    "- The report should be easy to run on a monthly basis\n",
    "- By making forward looking predictions the validity of this analysis can be assessed.\n",
    "- It may also be possible to look at if this analysis can generate a return (by incorporating options price data)\n",
    "- Report can then be shared with others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91cd70",
   "metadata": {},
   "source": [
    "## Section 01.1 : Importing Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Screen FTSE Aim Index Companies for bubbles, tickers may need \".L\" added onto them\n",
    "\n",
    "The a priori method of determining bubbles should still apply\n",
    "\n",
    "Yfinance module may have issues when trying to receive data for this many companies.\n",
    "\n",
    "Investigate whether there are other libraries which can do bulk downloads without having your IP Address blocked\n",
    "'''\n",
    "\n",
    "ftse_aim_df = pd.read_csv('FTSE_AIM_TICKERS.csv', encoding = \"iso-8859-1\")\n",
    "tickers = ftse_aim_df['Ticker'].tolist()\n",
    "stocks = ftse_aim_df['Name'].tolist()\n",
    "tickers_l = [t + \".L\" for t in tickers]\n",
    "\n",
    "df = np.array([stocks,tickers_l])\n",
    "print(\"The array of FTSE AIM stocks with their tickers is %s\" % df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For a trial run of the workbook, just two tickers will be used:\n",
    "'''\n",
    "insurance_tickers = pd.read_csv('Insurance_Companies.csv', encoding = \"utf-8\")\n",
    "\n",
    "insurance_names = insurance_tickers['Name'].to_list()\n",
    "insurance_stocks = insurance_tickers['Stock'].to_list()\n",
    "\n",
    "print(insurance_names)\n",
    "print(insurance_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72f110",
   "metadata": {},
   "source": [
    "## Section 01.2 : Importing Libraries and Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07314a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lppls_funcs_fix as lppls_funcs # Use my updated python file, with new calculation of LPPLS Confidence Indicators\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e70d5",
   "metadata": {},
   "source": [
    "**Now select the length of time to conduct the bubble analysis over**\n",
    "\n",
    "- Historical stock prices can be downloaded over a varying time period\n",
    "- Using a longer time period will slow down the run time of the model\n",
    "- Granularity of the dataset can be varied to download prices quoted: weekly, daily, hourly, etc.\n",
    "- For practicality reasons an hourly dataset of prices over a 1 to 3 month period should be a good compromise, which balances usefulness of the analysis and speed of run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_widget = widgets.RadioButtons(\n",
    "    options=['1mo', '2mo', '3mo','6mo','1y','2y'],\n",
    "    value = '1y', #defaults to 1 year\n",
    "    description='Months of historical data to use in analysis:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(duration_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Range of granularities: 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo\n",
    "\n",
    "granularity_widget = widgets.RadioButtons(\n",
    "    options=['1wk', '1d' ,'1h'], # hourly prices are likely unavailable for insurance stocks as they're not very liquid\n",
    "    value = '1d', #defaults to 1 point per day\n",
    "    description='Granularity of historical data, prices quoted:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(granularity_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299f92a",
   "metadata": {},
   "source": [
    "**Choose Stock to analse within the workbook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = \"BEZ.L\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe06eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data for a single stock\n",
    "# To download for multiple stocks enter \"SPY QQQ VWRP\"\n",
    "\n",
    "data = yf.download(insurance_stocks, period = duration_widget.value, interval = granularity_widget.value)\n",
    "time = [pd.Timestamp.toordinal(t1) for t1 in data.index]\n",
    "prices = [i[0] for i in data['Close'].values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d41fe",
   "metadata": {},
   "source": [
    "## Section 01.3 : Saving YFinance Download to CSV & Overwriting current CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6144ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_closes = pd.DataFrame(data['Close'])\n",
    "pd_closes_long = pd_closes.reset_index().melt(id_vars=['Date'], var_name='Ticker', value_name='Prices')\n",
    "pd_closes_long['Date'] = pd_closes_long['Date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63618655",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_long_data = pd.read_csv('Bubble_Data/price_data.csv')\n",
    "current_long_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963018eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([pd_closes_long, current_long_data])\n",
    "\n",
    "combined_data = combined_data.drop_duplicates(subset=['Date', 'Ticker']).reset_index(drop=True)\n",
    "\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a20e0",
   "metadata": {},
   "source": [
    "**Add new data to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_closes_long.to_csv('Bubble_Data/price_data.csv', index = False)\n",
    "combined_data.query(\"Ticker == 'AIG'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(insurance_stocks)):\n",
    "    globals()[f'var_{i:02}'] = None\n",
    "\n",
    "# Example: Assigning values to the dynamically created variables\n",
    "for i in range(0, len(insurance_stocks)):\n",
    "    t = combined_data[combined_data['Ticker'] == insurance_stocks[i]]['Date']\n",
    "    time = [pd.Timestamp(t1).toordinal() for t1 in t.values]\n",
    "    p = combined_data[combined_data['Ticker'] == insurance_stocks[i]]['Prices']\n",
    "    globals()[f'var_{i:02}'] = lppls_funcs.LPPLS(observations = np.array([time,p]))\n",
    "\n",
    "# Printing the values of the dynamically created variables\n",
    "for i in range(0, len(insurance_stocks)):\n",
    "    print(f'var_{i:02} =', globals()[f'var_{i:02}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91468243",
   "metadata": {},
   "source": [
    "## Section 02.1: Running the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fce73f",
   "metadata": {},
   "source": [
    "**Select how many stocks to run the model for**\n",
    "\n",
    "The initial run of the model will produce a best fit of the log period power law singularity curve for the time series data.\n",
    "\n",
    "At this stage you need to select how many individual stocks to run the next set of analysis for:\n",
    "- Select from the drop down below the number of stocks to compute the analysis for.\n",
    "- The plots will show the graphical outputs for a single stock for reference.\n",
    "- The time intensive part of the model is the calculation of the future bubble likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEARCHES = 25\n",
    "observations = np.array([time, prices])\n",
    "\n",
    "# Creates a new LPPLS model for a specific set of price and time series data\n",
    "lppls_model = lppls_funcs.LPPLS(observations=observations)\n",
    "#indicators_medium = lppls_model.compute_indicators(res_medium)\n",
    "\n",
    "# Fit the model to the data and get back the parameters\n",
    "tc, m, w, a, b, c, c1, c2, O, D = lppls_model.fit(MAX_SEARCHES)\n",
    "\n",
    "# Visualize the fit\n",
    "lppls_model.plot_fit()\n",
    "\n",
    "print(\"The critical time is: %s\" % pd.Timestamp.fromordinal(int(round(tc, 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959da943",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_fit = indicators_medium._fits[229][9]\n",
    "\n",
    "lppls_test = lppls_funcs.LPPLS(observations = observations)\n",
    "\n",
    "test_fit = lppls_test.lppls(\n",
    "    observations[0],\n",
    "    example_fit['tc'],\n",
    "    example_fit['m'],\n",
    "    example_fit['w'],\n",
    "    example_fit['a'],\n",
    "    example_fit['b'],\n",
    "    example_fit['c1'],\n",
    "    example_fit['c2']\n",
    ")\n",
    "\n",
    "print(example_fit['tc'])\n",
    "print(\"The example fit's critical time is: %s\" % pd.Timestamp.fromordinal(int(round(example_fit['tc'], 0))))\n",
    "print(example_fit['t2'])\n",
    "print(\"The example fit's window end time is: %s\" % pd.Timestamp.fromordinal(int(round(example_fit['t2'], 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53badbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_obs = [pd.Timestamp.fromordinal(d) for d in observations[0, :].astype('int32')]\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(14, 8))\n",
    "\n",
    "ax1.plot(t_obs, observations[1], label='price', color='black', linewidth=0.75)\n",
    "ax1.plot(t_obs, test_fit, label='lppls fit', color='blue', alpha=0.5)\n",
    "ax1.grid(which='major', axis='both', linestyle='-.')\n",
    "ax1.set_ylabel('ln(price)')\n",
    "ax1.set_xlabel('Date')\n",
    "plt.axvline(x=pd.Timestamp.fromordinal(int(round(example_fit['t2'], 0))), label='t2={}'.format(pd.Timestamp.fromordinal(int(round(example_fit['t2'],0)))), linestyle = '--', color = 'red')\n",
    "plt.axvline(x=pd.Timestamp.fromordinal(int(round(example_fit['tc'], 0))), label='tc={}'.format(pd.Timestamp.fromordinal(int(round(example_fit['tc'],0)))), linestyle = '--', color = 'green')\n",
    "plt.xticks(rotation=10)\n",
    "ax1.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b9b86",
   "metadata": {},
   "source": [
    "## Section 02.2 Computing Confidence Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72fce4",
   "metadata": {},
   "source": [
    "**Now compute the likelihood of a future bubble forming**\n",
    "\n",
    "- Select from the drop down whether you want to identify short or medium term bubbles in the stock prices.\n",
    "- Short term bubbles consider 1 to 6 months of historical data.\n",
    "- Medium term bubbles consider up to 18 months of historical data.\n",
    "- Identifying a long term bubble can be too time intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "res_medium = lppls_model.mp_compute_nested_fits(\n",
    "    workers=16,\n",
    "    window_size=90,\n",
    "    smallest_window_size=10, \n",
    "    outer_increment=1,\n",
    "    inner_increment=1, \n",
    "    max_searches=25\n",
    ")\n",
    "'''\n",
    "# In order to save run time res_medium is calculated once\n",
    "# Then saved using pickle library\n",
    "# Then imported again from pickle, as computing fits can take a long time\n",
    "\n",
    "indicators_medium\n",
    "\n",
    "lppls_model.plot_confidence_indicators(res_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214557e",
   "metadata": {},
   "source": [
    "## Section 02.3 Getting critical times of the bubbles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb0d3f",
   "metadata": {},
   "source": [
    "**The bubble critical time is the time at which the bubble peaks**\n",
    "\n",
    "- This is the likely time for when the price of the stock reaches its maximum/minimum for positive/negative bubbles\n",
    "- In this section we show a table of results for each stock, with the critical time, bubble size and scenario probability of the future development of the bubble.\n",
    "- You can also select the analysis below to show the extrapolated plot of the future stock price curve of the predicted bubble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a3452",
   "metadata": {},
   "source": [
    "**K Means Clustering**\n",
    "\n",
    "The k-means clustering calculation that is performed in the (t<sub>2</sub> - t<sub>1</sub>, t<sub>c</sub>-t<sub>2</sub>) space.\n",
    "The analysis is performed for a single value of t<sub>2</sub>, i.e on a single collection of fits extrapolated from the same day.\n",
    "This k-means clustering analysis is conducted on all the qualifying fits from that day.\n",
    "\n",
    "Each cluster will represent a different possible future scenario that is the best fit of the collection of predictions\n",
    "- For each cluster, calculate average of critical time (mu<sub>tc</sub>) and the standard deviation of time (sigma<sub>tc</sub>)\n",
    "- We calculate the scenario probability as the fraction of fits that fall into the cluster\n",
    "- Only the qualified fits get put into clusters\n",
    "\n",
    "https://ethz.ch/content/dam/ethz/special-interest/mtec/chair-of-entrepreneurial-risks-dam/documents/FCO/appendix-FCO-ETH-SIMAG.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e239cb",
   "metadata": {},
   "source": [
    "We would expect that we have more confidence in a bubble if it can be detected across all windows\n",
    "\n",
    "In our standard example we calculate the probabilities of bubbles at dates, considering data from a window of 120 to 30 days prior.\n",
    "We use an inner increment of 3 days, meaning the 90 day duration of the window is divided into 30 sub-windows\n",
    "Bubble pos_conf from the model is calculated as the share of positive bubbles (where b>0) which are qualified (have valid values for all parameters), divided by the number of positive bubbles in general.\n",
    "Why is the the share of positive bubbles not divided by the share of all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc04514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array with date, critical time, bubble likelihood\n",
    "\n",
    "# 1. Get positive bubble dates with at least 3 qualifying fits (so that k means clustering can be ran)\n",
    "qualifying_count = []\n",
    "for date in indicators_medium._fits:\n",
    "    qual_count = 0\n",
    "    for fit in date:\n",
    "        if fit['is_qualified'] == True:\n",
    "            qual_count += 1\n",
    "    qualifying_count.append(qual_count)\n",
    "\n",
    "fits = [count > 2 for count in qualifying_count]\n",
    "high_conf_df = indicators_medium[fits]\n",
    "\n",
    "# 2. append to matrix, a col with just the tc values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f19dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_bubble_times(df):\n",
    "    '''\n",
    "    Function takes a dataframe with all the confidence indicators for a dataset, \n",
    "    then will add an additional column with time-space coordinates for the qualifying positive\n",
    "    bubble fits.\n",
    "    \n",
    "    Inputs:\n",
    "    df (pandas dataframe), the compute_indicators() output\n",
    "    \n",
    "    Outputs:\n",
    "    df_out (pandas dataframe), the compute_indicators() output, with additional columns of 'time array'\n",
    "        and 'tc'.\n",
    "    \n",
    "        'time array' (array), column of arrays of all the coordinates of the qualifying\n",
    "            fits for the positive bubbles detected.\n",
    "    \n",
    "            Coordinates are in the (t2 - t1, tc - t2) space.\n",
    "                \n",
    "        'tc' (float), column with float values for the critical time, the time when the bubble peaks.\n",
    "    '''\n",
    "    fits = df._fits\n",
    "    vars = []\n",
    "    for dicts in fits:\n",
    "        t_list = []\n",
    "        total_fits = len(dicts)\n",
    "        for element in dicts:\n",
    "            if element['is_qualified'] == True and element['b'] < 0 :\n",
    "                t_list.append((element['t2'] - element['t1'], element['tc'] - element['t2']))\n",
    "        total_qualifying = len(t_list)\n",
    "        vars.append((total_qualifying ,t_list))\n",
    "    \n",
    "    compute_df = [*zip(*vars)]\n",
    "    df_out = df.copy()\n",
    "    df_out.loc[:,'qualifying_num'] = compute_df[0]\n",
    "    df_out.loc[:,'time_array'] = compute_df[1]\n",
    "    return(df_out)\n",
    "\n",
    "pos_array_df = positive_bubble_times(high_conf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cd278",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_array_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def get_clusters(max_clusters, df):\n",
    "    '''\n",
    "    Function analyses the predicted bubble scenarios produced for each qualifying fit of the data,\n",
    "    all qualifying fits of the data have a critical time (the time at which the bubble peaks), as\n",
    "    well as bubble start time t2, and the start time of the historical data period used to fit the\n",
    "    curve t1.\n",
    "\n",
    "    Qualifying fits when plotted in the (t2- t1, tc-t2) space will appear as points near eachother\n",
    "    if the predicted bubbles peak at the same time in the future.\n",
    "\n",
    "    Hence we use k means clustering to figure out how many different clusters (groups of fits which\n",
    "    represent the same future bubble\n",
    "\n",
    "    Inputs:\n",
    "    df (pandas dataframe), the time_arrays values from the compute_indicators() function output. \n",
    "\n",
    "    max_clusters (integer), the number of clusters to identify fits\n",
    "\n",
    "    Outputs:\n",
    "    df_output (dataframe), a dataframe with the same number of rows as df. It has three columns:\n",
    "    \n",
    "        clusters (integer), the number of clusters detected, for which the qualifying fits predict similar bubbles.\n",
    "\n",
    "        cluster_coords (array), an array of tuples, the coordinates in the (t2 - t1, tc - t2) space that are\n",
    "            located within the largest cluster. If multiple clusters have the same number of points within\n",
    "            them, then array is made up of multiple sub-arrays, each a group of coordinates.\n",
    "    '''\n",
    "    range_clusters = range(2, max_clusters)\n",
    "    cluster_num = []\n",
    "    coords = []\n",
    "    for array in df:\n",
    "        ###\n",
    "        # NOTE : Include check here that number of qualifying fits in 'array' is >= max_clusters\n",
    "        ###\n",
    "        \n",
    "        if len(array) < max_clusters:\n",
    "            raise ValueError(\"One time array only has %s qualifying fits, the minimum is %s\" % (len(array), max_clusters))\n",
    "        ###\n",
    "        silhouette_avg = []\n",
    "        for n in range_clusters:\n",
    "            kmeans = KMeans(n_clusters=n, random_state=0, n_init=\"auto\").fit(array)\n",
    "            silhouette_avg.append(silhouette_score(array, kmeans.labels_))\n",
    "\n",
    "        silhouette_index = silhouette_avg.index(max(silhouette_avg))\n",
    "        silhouette_optimum_n = silhouette_index + 2 # as minimum number of clusters in k means analysis is 2\n",
    "        cluster_num.append(silhouette_optimum_n)\n",
    "        \n",
    "        # With number of clusters, we now get the coordinates within the largest cluster\n",
    "        kmeans_opt = KMeans(n_clusters = silhouette_optimum_n, random_state = 0, n_init = \"auto\").fit(array)\n",
    "        #1. calculate largest cluster\n",
    "        cluster_ints = kmeans_opt.labels_.tolist()\n",
    "        all_modes = [i for i in set(cluster_ints) if cluster_ints.count(i) == max(map(cluster_ints.count,cluster_ints))]\n",
    "\n",
    "        #2. get index of largest cluster, from kmeans.labels_\n",
    "        #3. get coords if (cluster index == largest cluster index)\n",
    "        multiple_modes = []\n",
    "        for j in all_modes:\n",
    "            coords_i = [array[i] for i in range(0,len(array)) if kmeans_opt.labels_[i] == j]\n",
    "            multiple_modes.append(coords_i)\n",
    "        coords.append(multiple_modes)\n",
    "\n",
    "    df_output = pd.DataFrame(\n",
    "        {'clusters' : cluster_num,\n",
    "         'cluster_coords' : coords\n",
    "        },\n",
    "        index = df.index.values.tolist()\n",
    "    )\n",
    "    return(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44734dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = get_clusters(3, pos_array_df.time_array)\n",
    "combined_df = pd.concat([pos_array_df, cluster_df], axis = 1)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cacde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mu tc and sigma tc\n",
    "\n",
    "def calculate_averages(cluster_dfs):\n",
    "    '''\n",
    "    Function calculates the time into the future the critical time of the bubble is expected to occur at, as well\n",
    "    as the error on the estimate of the critical time.\n",
    "\n",
    "    The bubble is expected to peak at tc, the average of all forecasts that are qualifying fits, is taken as \n",
    "    mu_tc_minus_t2.\n",
    "\n",
    "    The error on this estimate of the critical time minus current time, is taken as sigma, the standard deviation\n",
    "    of all the qualifying fits for each bubble cluster.\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "    Outputs:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cluster_coords = cluster_dfs['cluster_coords']\n",
    "    mu_vals = []\n",
    "    sigma_vals = []\n",
    "    for row in cluster_coords:\n",
    "        if(len(row) == 1):\n",
    "            mu_tc_minus_t2 = sum(v[1] for v in row[0]) / len(row[0])\n",
    "            mu_squared = sum(v[1]**2 for v in row[0]) / len(row[0])\n",
    "            variance = mu_squared - mu_tc_minus_t2**2\n",
    "            sigma = math.sqrt(variance)\n",
    "            mu_vals.append(mu_tc_minus_t2)\n",
    "            sigma_vals.append(sigma)\n",
    "        else:\n",
    "            averages = []\n",
    "            sigmas = []\n",
    "            for cluster in row:\n",
    "                mu_tc_minus_t2 = sum(v[1] for v in cluster) / len(cluster)\n",
    "                mu_squared = sum(v[1]**2 for v in cluster) / len(cluster)\n",
    "                variance = mu_squared - mu_tc_minus_t2**2\n",
    "                sigma = math.sqrt(variance)\n",
    "                averages.append(mu_tc_minus_t2)\n",
    "                sigmas.append(sigma)\n",
    "            mu_vals.append(averages)\n",
    "            sigma_vals.append(sigmas)\n",
    "    df_stats = pd.DataFrame(\n",
    "        {\n",
    "            'mu_vals' : mu_vals,\n",
    "            'sigma_vals' : sigma_vals\n",
    "        },\n",
    "        index = cluster_dfs.index.values.tolist()\n",
    "    )\n",
    "    return(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_averages(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79733183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_probability (df):\n",
    "    '''\n",
    "    Takes a dataframe...\n",
    "    \n",
    "    Inputs\n",
    "\n",
    "    Outputs\n",
    "    '''\n",
    "    df['num_largest_clusters'] = df['cluster_coords'].apply(\n",
    "        lambda x: max(len(i) for i in x)\n",
    "    )\n",
    "    df['scenario_probability'] = df.num_largest_clusters / df.qualifying_num\n",
    "    df['overall_probability'] = df.scenario_probability * df.pos_conf\n",
    "    return(df)\n",
    "\n",
    "scenario_probability(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b629c2f",
   "metadata": {},
   "source": [
    "**Calculate a pdf distribution for critical times**\n",
    "\n",
    "- Critical time estimates using the full set of historical data can give a pdf of the distribution of the critical time\n",
    "- Each point in time forecast (at a given t2), can give an estimate of the bubble critical time mu_tc, and the std deviation sigma_tc\n",
    "- 1. create a plot of the k means clustering to vertify that the values of mu_tc and sigma_tc are sensible\n",
    "  2. calculate the CDF of the forecast for the critical times\n",
    "  3. Add this plot to the chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad897f4d",
   "metadata": {},
   "source": [
    "**New function**\n",
    "\n",
    "- Choose range of dates\n",
    "- Function then takes all bubble forecasts within that period\n",
    "- Combines them together to create an overall ensemble bubble forecast\n",
    "- E.g if in a 10 day period, 6 days have positive bubble probabilities of 50%, then this may be a good indicator of potential bubble formation\n",
    "- Rather than just using a single day's forecast, we give credence to having multiple forecasts on different run dates also overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb41e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_bubble_clusters (df, index):\n",
    "    '''\n",
    "    A function to plot the qualifying positive bubble fits within the (t2- t1, tc-t2) space.\n",
    "    K means clusters will appear as points near eachother if the predicted bubbles peak at the same time in the future.\n",
    "\n",
    "    Inputs\n",
    "    df (dataframe) :\n",
    "        The dataframe which is the output of the 'get_clusters' function\n",
    "    Outputs\n",
    "    plt (matplotlib pyplot object)\n",
    "        Returns a plot of qualifying bubble fits for a single time 't2' in the (t2-t1, tc-t2) space\n",
    "        Plot also shows the coordinates of the centres of the clusters identified\n",
    "    '''\n",
    "    x = [item[0] for item in df.time_array[index]]\n",
    "    y = [item[1] for item in df.time_array[index]]\n",
    "    plt.scatter(\n",
    "        x,\n",
    "        y,\n",
    "        name = 'All fits'\n",
    "    )\n",
    "    plt.scatter(\n",
    "        x = [item[0] for item in df.cluster_coords[index]],\n",
    "        y = [item[1] for item in df.cluster_coords[index]],\n",
    "        name = 'Cluster Centres'\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return ()\n",
    "\n",
    "def view_all_bubbles (df):\n",
    "    indices = df.index.tolist()\n",
    "    for i in indices:\n",
    "        view_bubble_clusters(df, i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c029dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_all_bubbles(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_min_t2 = [tupl[1] for row in combined_df.time_array for tupl in row]\n",
    "t2_min_t1 = [tupl[0] for row in combined_df.time_array for tupl in row]\n",
    "\n",
    "plt.scatter(t2_min_t1,\n",
    "            tc_min_t2)\n",
    "plt.show()\n",
    "\n",
    "combined_df.assign(qual_t2 = lambda combined_df.time_array + combined_df.time) #check\n",
    "\n",
    "tc_qual = [pd.Timestamp.fromordinal(int(round(dicti['tc'],0))) for row in combined_df._fits for dicti in row if dicti['is_qualified'] == True]\n",
    "t2_qual = [pd.Timestamp.fromordinal(int(round(dicti['t2'],0))) for row in combined_df._fits for dicti in row if dicti['is_qualified'] == True]\n",
    "\n",
    "plt.scatter(\n",
    "    t2_qual, tc_qual)\n",
    "plt.axvline(x=pd.Timestamp.fromordinal(int(round(time[-1],0))), label='tc={}'.format(time[-1]), linestyle = '--', color = 'green')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69f49f",
   "metadata": {},
   "source": [
    "**Create and save down a table**\n",
    "\n",
    "- Save down a run of the bubble index at current date\n",
    "- Append new rows of price data onto existing dataframe of price data\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
